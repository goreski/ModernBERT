{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:958: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/root/miniconda/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:1017: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Optional, cast\n",
    "\n",
    "import src.evals.data as data_module\n",
    "import src.hf_bert as hf_bert_module\n",
    "import src.mosaic_bert as mosaic_bert_module\n",
    "import src.flex_bert as flex_bert_module\n",
    "import transformers\n",
    "from composer import Trainer, algorithms, Evaluator\n",
    "from composer.callbacks import LRMonitor, MemoryMonitor, OptimizerMonitor, RuntimeEstimator, SpeedMonitor\n",
    "from composer.core.types import Dataset\n",
    "from composer.loggers import WandBLogger\n",
    "from composer.optim import DecoupledAdamW\n",
    "from composer.optim.scheduler import (\n",
    "    ConstantWithWarmupScheduler,\n",
    "    CosineAnnealingWithWarmupScheduler,\n",
    "    LinearWithWarmupScheduler,\n",
    ")\n",
    "from src.scheduler import WarmupStableDecayScheduler\n",
    "from composer.utils import dist, reproducibility\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1[{\"variableName\": \"ID_TO_MEANING\", \"type\": \"dictionary\", \"supportedEngines\": [\"pandas\"], \"isLocalVariable\": true, \"rawType\": \"builtins.dict\"}, {\"variableName\": \"NULL\", \"type\": \"unknown\", \"supportedEngines\": [\"pandas\"], \"isLocalVariable\": true, \"rawType\": \"_pydevd_bundle.pydevd_constants.Null\"}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def build_my_dataloader(cfg: DictConfig, device_batch_size: int):\n",
    "    \"\"\"Create a dataloader for classification.\n",
    "\n",
    "    **Modify this function to train on your own dataset!**\n",
    "\n",
    "    This function is provided as a starter code to simplify fine-tuning a BERT\n",
    "    classifier on your dataset. We'll use the dataset for QNLI (one of the\n",
    "    GLUE tasks) as a demonstration.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): An omegaconf config that houses all the configuration\n",
    "            variables needed to instruct dataset/dataloader creation.\n",
    "        device_batch_size (int): The size of the batches that the dataloader\n",
    "            should produce.\n",
    "\n",
    "    Returns:\n",
    "        dataloader: A dataloader set up for use of the Composer Trainer.\n",
    "    \"\"\"\n",
    "    # As a demonstration, we're using the QNLI dataset from the GLUE suite\n",
    "    # of tasks.\n",
    "    #\n",
    "    # Note: We create our dataset using the `data_module.create_glue_dataset` utility\n",
    "    #   defined in `./src/glue/data.py`. If you inspect that code, you'll see\n",
    "    #   that we're taking some extra steps so that our dataset yields examples\n",
    "    #   that follow a particular format. In particular, the raw text is\n",
    "    #   tokenized and some of the data columns are removed. The result is that\n",
    "    #   each example is a dictionary with the following:\n",
    "    #\n",
    "    #     - 'input_ids': the tokenized raw text\n",
    "    #     - 'label': the target class that the text belongs to\n",
    "    #     - 'attention_mask': a list of 1s and 0s to indicate padding\n",
    "    #\n",
    "    # When you set up your own dataset, it should handle tokenization to yield\n",
    "    # examples with a similar structure!\n",
    "    #\n",
    "    # REPLACE THIS WITH YOUR OWN DATASET:\n",
    "    dataset = data_module.create_glue_dataset(\n",
    "        task=\"qnli\",\n",
    "        split=cfg.split,\n",
    "        tokenizer_name=cfg.tokenizer_name,\n",
    "        max_seq_length=cfg.max_seq_len,\n",
    "    )\n",
    "\n",
    "    dataset = cast(Dataset, dataset)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        # As an alternative to formatting the examples inside the dataloader,\n",
    "        # you can write a custom data collator to do that instead.\n",
    "        collate_fn=transformers.default_data_collator,\n",
    "        batch_size=device_batch_size,\n",
    "        sampler=dist.get_sampler(dataset, drop_last=cfg.drop_last, shuffle=cfg.shuffle),\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=cfg.get(\"pin_memory\", True),\n",
    "        prefetch_factor=cfg.get(\"prefetch_factor\", 2),\n",
    "        persistent_workers=cfg.get(\"persistent_workers\", True),\n",
    "        timeout=cfg.get(\"timeout\", 0),\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "yaml_path, args_list = \"yamls/test/sequence_classification.yaml\", sys.argv[2:]\n",
    "with open(\"yamls/defaults.yaml\") as f:\n",
    "    default_cfg = om.load(f)\n",
    "with open(yaml_path) as f:\n",
    "    yaml_cfg = om.load(f)\n",
    "cli_cfg = om.from_cli(args_list)\n",
    "cfg = om.merge(default_cfg, yaml_cfg, cli_cfg)\n",
    "cfg = cast(DictConfig, cfg)  # for type checking\n",
    "\n",
    "train_loader = build_my_dataloader(\n",
    "    cfg.train_loader,\n",
    "    cfg.global_train_batch_size // dist.get_world_size(),\n",
    ")\n",
    "\n",
    "#get one data sample from the train_loader\n",
    "data = next(iter(train_loader))\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
