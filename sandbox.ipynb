{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:958: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:1017: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Optional, cast\n",
    "\n",
    "import src.evals.data as data_module\n",
    "import src.hf_bert as hf_bert_module\n",
    "import src.mosaic_bert as mosaic_bert_module\n",
    "import src.flex_bert as flex_bert_module\n",
    "import transformers\n",
    "from composer import Trainer, algorithms, Evaluator\n",
    "from composer.callbacks import LRMonitor, MemoryMonitor, OptimizerMonitor, RuntimeEstimator, SpeedMonitor\n",
    "from composer.core.types import Dataset\n",
    "from composer.loggers import WandBLogger\n",
    "from composer.optim import DecoupledAdamW\n",
    "from composer.optim.scheduler import (\n",
    "    ConstantWithWarmupScheduler,\n",
    "    CosineAnnealingWithWarmupScheduler,\n",
    "    LinearWithWarmupScheduler,\n",
    ")\n",
    "from src.scheduler import WarmupStableDecayScheduler\n",
    "from composer.utils import dist, reproducibility\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from glue dataset:\n",
      "{'question': 'When did the third Digimon series begin?', 'sentence': 'Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.', 'label': 1, 'idx': 0}\n",
      "{'question': 'Which missile batteries often have individual launchers several kilometres from one another?', 'sentence': 'When MANPADS is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.', 'label': 1, 'idx': 1}\n",
      "{'question': \"What two things does Popper argue Tarski's theory involves in an evaluation of truth?\", 'sentence': 'He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.', 'label': 0, 'idx': 2}\n",
      "{'question': 'What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians?', 'sentence': 'On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854.', 'label': 0, 'idx': 3}\n",
      "{'question': 'What famous palace is located in London?', 'sentence': \"London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0Â° longitude, and GMT).\", 'label': 1, 'idx': 4}\n",
      "dict_keys(['labels', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'])\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]), 'idx': tensor([104315,  74962,  80369,  99703,  15104,  60806,  11905,  75662,  22350,\n",
      "         32578,  31140,  16361,  24973,  90682,  91633,  87791]), 'input_ids': tensor([[ 101, 2129, 2146,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 5274,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 2055, 2129,  ...,    0,    0,    0],\n",
      "        [ 101, 2108, 2485,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "def build_my_dataloader(cfg: DictConfig, device_batch_size: int):\n",
    "    \"\"\"Create a dataloader for classification.\n",
    "\n",
    "    **Modify this function to train on your own dataset!**\n",
    "\n",
    "    This function is provided as a starter code to simplify fine-tuning a BERT\n",
    "    classifier on your dataset. We'll use the dataset for QNLI (one of the\n",
    "    GLUE tasks) as a demonstration.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): An omegaconf config that houses all the configuration\n",
    "            variables needed to instruct dataset/dataloader creation.\n",
    "        device_batch_size (int): The size of the batches that the dataloader\n",
    "            should produce.\n",
    "\n",
    "    Returns:\n",
    "        dataloader: A dataloader set up for use of the Composer Trainer.\n",
    "    \"\"\"\n",
    "    # As a demonstration, we're using the QNLI dataset from the GLUE suite\n",
    "    # of tasks.\n",
    "    #\n",
    "    # Note: We create our dataset using the `data_module.create_glue_dataset` utility\n",
    "    #   defined in `./src/glue/data.py`. If you inspect that code, you'll see\n",
    "    #   that we're taking some extra steps so that our dataset yields examples\n",
    "    #   that follow a particular format. In particular, the raw text is\n",
    "    #   tokenized and some of the data columns are removed. The result is that\n",
    "    #   each example is a dictionary with the following:\n",
    "    #\n",
    "    #     - 'input_ids': the tokenized raw text\n",
    "    #     - 'label': the target class that the text belongs to\n",
    "    #     - 'attention_mask': a list of 1s and 0s to indicate padding\n",
    "    #\n",
    "    # When you set up your own dataset, it should handle tokenization to yield\n",
    "    # examples with a similar structure!\n",
    "    #\n",
    "    # REPLACE THIS WITH YOUR OWN DATASET:\n",
    "    dataset = data_module.create_glue_dataset(\n",
    "        task=\"qnli\",\n",
    "        split=cfg.split,\n",
    "        tokenizer_name=cfg.tokenizer_name,\n",
    "        max_seq_length=cfg.max_seq_len,\n",
    "    )\n",
    "\n",
    "    dataset = cast(Dataset, dataset)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        # As an alternative to formatting the examples inside the dataloader,\n",
    "        # you can write a custom data collator to do that instead.\n",
    "        collate_fn=transformers.default_data_collator,\n",
    "        batch_size=device_batch_size,\n",
    "        sampler=dist.get_sampler(dataset, drop_last=cfg.drop_last, shuffle=cfg.shuffle),\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=cfg.get(\"pin_memory\", True),\n",
    "        prefetch_factor=cfg.get(\"prefetch_factor\", 2),\n",
    "        persistent_workers=cfg.get(\"persistent_workers\", True),\n",
    "        timeout=cfg.get(\"timeout\", 0),\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "yaml_path, args_list = \"yamls/test/sequence_classification.yaml\", sys.argv[2:]\n",
    "with open(\"yamls/defaults.yaml\") as f:\n",
    "    default_cfg = om.load(f)\n",
    "with open(yaml_path) as f:\n",
    "    yaml_cfg = om.load(f)\n",
    "cli_cfg = om.from_cli(args_list)\n",
    "cfg = om.merge(default_cfg, yaml_cfg, cli_cfg)\n",
    "cfg = cast(DictConfig, cfg)  # for type checking\n",
    "\n",
    "train_loader = build_my_dataloader(\n",
    "    cfg.train_loader,\n",
    "    cfg.global_train_batch_size // dist.get_world_size(),\n",
    ")\n",
    "\n",
    "#get one data sample from the train_loader\n",
    "data = next(iter(train_loader))\n",
    "# Print data keys only\n",
    "print(data.keys())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer Debug Info:\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 1:\n",
      "Input sentence: 5<DEC>7655\n",
      "Label: 0\n",
      "Token IDs: [101, 1019, 30522, 6146, 24087, 102]\n",
      "Decoded text: [CLS] 5 <DEC> 7655 [SEP]\n",
      "Individual tokens: ['[CLS]', '5', '<DEC>', '76', '##55', '[SEP]']\n",
      "\n",
      "Example 2:\n",
      "Input sentence: 4<DEC>23245\n",
      "Label: 1\n",
      "Token IDs: [101, 1018, 30522, 20666, 19961, 102]\n",
      "Decoded text: [CLS] 4 <DEC> 23245 [SEP]\n",
      "Individual tokens: ['[CLS]', '4', '<DEC>', '232', '##45', '[SEP]']\n",
      "\n",
      "Example 3:\n",
      "Input sentence: 5<DEC>7655\n",
      "Label: 0\n",
      "Token IDs: [101, 1019, 30522, 6146, 24087, 102]\n",
      "Decoded text: [CLS] 5 <DEC> 7655 [SEP]\n",
      "Individual tokens: ['[CLS]', '5', '<DEC>', '76', '##55', '[SEP]']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from generate_dataset import generate_synthetic_dataset\n",
    "\n",
    "# Configuration parameters\n",
    "decimal_token = \"<DEC>\"\n",
    "\n",
    "cfg = {}\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "df = generate_synthetic_dataset(\n",
    "    n_samples=cfg.get(\"n_samples\", 100),\n",
    "    n_continuous_features=cfg.get(\"n_continuous_features\", 15),\n",
    "    n_discrete_features=cfg.get(\"n_discrete_features\", 15),\n",
    "    n_classes=cfg.get(\"n_classes\", 2),\n",
    "    class_distribution=cfg.get(\"class_distribution\", [0.8, 0.2]),\n",
    "    n_bins=cfg.get(\"n_bins\", 10),\n",
    "    n_redundant=cfg.get(\"n_redundant\", 5),\n",
    "    n_noisy=cfg.get(\"n_noisy\", 20),\n",
    "    class_sep=cfg.get(\"class_sep\", 0.1),\n",
    ")\n",
    "\n",
    "# Change structure to \"sentence\", \"label\" and \"idx\"\n",
    "# All columns except the last one are features and they are concatenated to form a sentence\n",
    "# The last column is the label\n",
    "df['sentence'] = df.drop(columns=['label']).apply(lambda x: ' '.join([f\"{val}\".replace('.', decimal_token) for val in x]), axis=1)\n",
    "\n",
    "# Create dummy sentence based on label: if 1 then \"4.23245\", if 0 then \"5.7655\"\n",
    "df['sentence'] = df['label'].apply(lambda x: f\"4{decimal_token}23245\" if x == 1 else f\"5{decimal_token}7655\")\n",
    "\n",
    "# Reorder columns and add index\n",
    "df = df[['sentence', 'label']]\n",
    "df['idx'] = df.index\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer.add_tokens([decimal_token])\n",
    "\n",
    "# Add a padding token if it doesn't already exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  # Use EOS token as padding token for GPT-2\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_dataset = tokenizer(\n",
    "    df['sentence'].tolist(),  # Ensure this is a list of strings\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Print input sentence and tokenization results\n",
    "print(\"\\nTokenizer Debug Info:\")\n",
    "print(\"-\" * 50)\n",
    "# Print first 3 examples\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input sentence: {df['sentence'].iloc[i]}\")\n",
    "    print(f\"Label: {df['label'].iloc[i]}\")\n",
    "    \n",
    "    # Get tokenized ids for this example\n",
    "    tokens = tokenizer.encode(df['sentence'].iloc[i])\n",
    "    print(f\"Token IDs: {tokens}\")\n",
    "    \n",
    "    # Decode back to string to verify tokenization\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"Decoded text: {decoded}\")\n",
    "    \n",
    "    # Print individual tokens\n",
    "    tokens_list = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    print(f\"Individual tokens: {tokens_list}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "labels = df['label'].tolist()\n",
    "custom_dataset = CustomDataset(tokenized_dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import itertools\n",
    "\n",
    "def create_extended_letter_mapping(df, label_column):\n",
    "    \"\"\"\n",
    "    Create a mapping from integer values to unique letters for each column in the dataframe, excluding the label column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        label_column (str): The name of the label column.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are column names and values are mappings from integers to unique letters.\n",
    "    \"\"\"\n",
    "    letter_mapping = {}\n",
    "    letters = string.ascii_uppercase  # Use uppercase letters A-Z\n",
    "    \n",
    "    # Generate combinations of letters (e.g., A, B, ..., Z, AA, AB, ..., ZZ, AAA, AAB, ...)\n",
    "    max_length = 3  # Adjust this value if needed to handle more unique values\n",
    "    extended_letters = [''.join(comb) for comb in itertools.chain.from_iterable(itertools.product(letters, repeat=i) for i in range(1, max_length + 1))]\n",
    "    \n",
    "    letter_index = 0  # To keep track of the starting index for each column's unique letters\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col == label_column:\n",
    "            continue  # Skip the label column\n",
    "        unique_values = df[col].unique()\n",
    "        if len(unique_values) > len(extended_letters) - letter_index:\n",
    "            raise ValueError(f\"Too many unique values in column {col} to map to extended letters.\")\n",
    "        \n",
    "        # Assign a unique subset of letters to this column\n",
    "        column_letters = extended_letters[letter_index:letter_index + len(unique_values)]\n",
    "        mapping = {val: column_letters[i] for i, val in enumerate(unique_values)}\n",
    "        letter_mapping[col] = mapping\n",
    "        \n",
    "        # Update the starting index for the next column\n",
    "        letter_index += len(unique_values)\n",
    "    \n",
    "    print(\"Letter mapping:\")\n",
    "    print(letter_mapping)\n",
    "    \n",
    "    return letter_mapping\n",
    "\n",
    "def transform_discrete_to_letters(df, letter_mapping):\n",
    "    \"\"\"\n",
    "    Transform discrete features from integer encoding to letter encoding, excluding the label column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        letter_mapping (dict): The mapping from integers to letters for each column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed dataframe.\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    for col, mapping in letter_mapping.items():\n",
    "        df_transformed[col] = df_transformed[col].map(mapping)\n",
    "    \n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 16)\n",
      "Letter mapping:\n",
      "{'discrete_feature_0': {3: 'A', 1: 'B', 4: 'C', 7: 'D', 5: 'E', 6: 'F', 2: 'G', 9: 'H', 8: 'I', 0: 'J'}, 'discrete_feature_1': {3: 'K', 5: 'L', 4: 'M', 8: 'N', 6: 'O', 7: 'P', 2: 'Q', 1: 'R', 0: 'S', 9: 'T'}, 'discrete_feature_2': {6: 'U', 4: 'V', 5: 'W', 1: 'X', 9: 'Y', 3: 'Z', 2: 'AA', 0: 'AB', 7: 'AC', 8: 'AD'}, 'discrete_feature_3': {4: 'AE', 2: 'AF', 1: 'AG', 6: 'AH', 8: 'AI', 7: 'AJ', 5: 'AK', 3: 'AL', 9: 'AM', 0: 'AN'}, 'discrete_feature_4': {5: 'AO', 3: 'AP', 4: 'AQ', 1: 'AR', 7: 'AS', 6: 'AT', 9: 'AU', 8: 'AV', 2: 'AW', 0: 'AX'}, 'discrete_feature_5': {6: 'AY', 3: 'AZ', 4: 'BA', 5: 'BB', 0: 'BC', 2: 'BD', 8: 'BE', 7: 'BF', 1: 'BG', 9: 'BH'}, 'discrete_feature_6': {7: 'BI', 3: 'BJ', 6: 'BK', 4: 'BL', 1: 'BM', 5: 'BN', 2: 'BO', 0: 'BP', 9: 'BQ', 8: 'BR'}, 'discrete_feature_7': {5: 'BS', 3: 'BT', 2: 'BU', 4: 'BV', 6: 'BW', 7: 'BX', 8: 'BY', 0: 'BZ', 9: 'CA', 1: 'CB'}, 'discrete_feature_8': {3: 'CC', 6: 'CD', 7: 'CE', 5: 'CF', 2: 'CG', 4: 'CH', 0: 'CI', 9: 'CJ', 1: 'CK', 8: 'CL'}, 'discrete_feature_9': {5: 'CM', 7: 'CN', 8: 'CO', 6: 'CP', 3: 'CQ', 4: 'CR', 9: 'CS', 0: 'CT', 1: 'CU', 2: 'CV'}, 'discrete_feature_10': {4: 'CW', 1: 'CX', 7: 'CY', 6: 'CZ', 5: 'DA', 2: 'DB', 3: 'DC', 9: 'DD', 8: 'DE', 0: 'DF'}, 'discrete_feature_11': {3: 'DG', 6: 'DH', 4: 'DI', 5: 'DJ', 1: 'DK', 8: 'DL', 7: 'DM', 9: 'DN', 2: 'DO', 0: 'DP'}, 'discrete_feature_12': {5: 'DQ', 6: 'DR', 4: 'DS', 3: 'DT', 9: 'DU', 8: 'DV', 1: 'DW', 7: 'DX', 0: 'DY', 2: 'DZ'}, 'discrete_feature_13': {1: 'EA', 4: 'EB', 3: 'EC', 2: 'ED', 5: 'EE', 7: 'EF', 0: 'EG', 9: 'EH', 6: 'EI'}, 'discrete_feature_14': {5: 'EJ', 4: 'EK', 3: 'EL', 7: 'EM', 2: 'EN', 6: 'EO', 0: 'EP', 1: 'EQ', 9: 'ER'}}\n",
      "\n",
      "Tokenizer Debug Info:\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 1:\n",
      "Input sentence: A K U AE AO AY BI BS CC CM CW DG DQ EA EJ\n",
      "Label: 1\n",
      "Token IDs: [101, 1037, 1047, 1057, 29347, 20118, 1037, 2100, 12170, 18667, 10507, 4642, 19296, 1040, 2290, 25410, 19413, 1041, 3501, 102]\n",
      "Decoded text: [CLS] a k u ae ao ay bi bs cc cm cw dg dq ea ej [SEP]\n",
      "Individual tokens: ['[CLS]', 'a', 'k', 'u', 'ae', 'ao', 'a', '##y', 'bi', 'bs', 'cc', 'cm', 'cw', 'd', '##g', 'dq', 'ea', 'e', '##j', '[SEP]']\n",
      "\n",
      "Example 2:\n",
      "Input sentence: B L V AF AP AZ BI BT CC CN CX DH DR EB EK\n",
      "Label: 1\n",
      "Token IDs: [101, 1038, 1048, 1058, 21358, 9706, 17207, 12170, 18411, 10507, 27166, 1039, 2595, 28144, 2852, 1041, 2497, 23969, 102]\n",
      "Decoded text: [CLS] b l v af ap az bi bt cc cn cx dh dr eb ek [SEP]\n",
      "Individual tokens: ['[CLS]', 'b', 'l', 'v', 'af', 'ap', 'az', 'bi', 'bt', 'cc', 'cn', 'c', '##x', 'dh', 'dr', 'e', '##b', 'ek', '[SEP]']\n",
      "\n",
      "Example 3:\n",
      "Input sentence: C M W AG AQ BA BJ BS CC CO CW DI DS EB EL\n",
      "Label: 0\n",
      "Token IDs: [101, 1039, 1049, 1059, 12943, 1037, 4160, 8670, 1038, 3501, 18667, 10507, 2522, 19296, 4487, 16233, 1041, 2497, 3449, 102]\n",
      "Decoded text: [CLS] c m w ag aq ba bj bs cc co cw di ds eb el [SEP]\n",
      "Individual tokens: ['[CLS]', 'c', 'm', 'w', 'ag', 'a', '##q', 'ba', 'b', '##j', 'bs', 'cc', 'co', 'cw', 'di', 'ds', 'e', '##b', 'el', '[SEP]']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate the synthetic dataset\n",
    "df = generate_synthetic_dataset(\n",
    "    n_samples=cfg.get(\"n_samples\", 100),\n",
    "    n_continuous_features=cfg.get(\"n_continuous_features\", 0),  # Set to 0 since all features are discrete\n",
    "    n_discrete_features=cfg.get(\"n_discrete_features\", 15),\n",
    "    n_classes=cfg.get(\"n_classes\", 2),\n",
    "    class_distribution=cfg.get(\"class_distribution\", [0.8, 0.2]),\n",
    "    n_bins=cfg.get(\"n_bins\", 10),\n",
    "    n_redundant=cfg.get(\"n_redundant\", 0),\n",
    "    n_noisy=cfg.get(\"n_noisy\", 0),\n",
    "    class_sep=cfg.get(\"class_sep\", 0.1),\n",
    ")\n",
    "\n",
    "# Define the label column\n",
    "label_column = 'label'\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Create extended letter mapping for all columns except the label column\n",
    "letter_mapping = create_extended_letter_mapping(df, label_column)\n",
    "\n",
    "# Transform all columns to letter encoding except the label column\n",
    "df_transformed = transform_discrete_to_letters(df.drop(columns=[label_column]), letter_mapping)\n",
    "\n",
    "# Add the label column back to the transformed dataframe\n",
    "df_transformed[label_column] = df[label_column]\n",
    "\n",
    "# Change structure to \"sentence\", \"label\" and \"idx\"\n",
    "# All columns except the last one are features and they are concatenated to form a sentence\n",
    "# The last column is the label\n",
    "df_transformed['sentence'] = df_transformed.drop(columns=[label_column]).apply(lambda x: ' '.join([str(val) for val in x]), axis=1)\n",
    "\n",
    "# Reorder columns and add index\n",
    "df_transformed = df_transformed[['sentence', label_column]]\n",
    "df_transformed['idx'] = df_transformed.index\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "\n",
    "# Add a padding token if it doesn't already exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  # Use EOS token as padding token for GPT-2\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_dataset = tokenizer(\n",
    "    df_transformed['sentence'].tolist(),  # Ensure this is a list of strings\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Print input sentence and tokenization results\n",
    "print(\"\\nTokenizer Debug Info:\")\n",
    "print(\"-\" * 50)\n",
    "# Print first 3 examples\n",
    "for i in range(min(3, len(df_transformed))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input sentence: {df_transformed['sentence'].iloc[i]}\")\n",
    "    print(f\"Label: {df_transformed[label_column].iloc[i]}\")\n",
    "    \n",
    "    # Get tokenized ids for this example\n",
    "    tokens = tokenizer.encode(df_transformed['sentence'].iloc[i])\n",
    "    print(f\"Token IDs: {tokens}\")\n",
    "    \n",
    "    # Decode back to string to verify tokenization\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"Decoded text: {decoded}\")\n",
    "    \n",
    "    # Print individual tokens\n",
    "    tokens_list = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    print(f\"Individual tokens: {tokens_list}\")\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
