{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:958: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:1017: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Optional, cast\n",
    "\n",
    "import src.evals.data as data_module\n",
    "import src.hf_bert as hf_bert_module\n",
    "import src.mosaic_bert as mosaic_bert_module\n",
    "import src.flex_bert as flex_bert_module\n",
    "import transformers\n",
    "from composer import Trainer, algorithms, Evaluator\n",
    "from composer.callbacks import LRMonitor, MemoryMonitor, OptimizerMonitor, RuntimeEstimator, SpeedMonitor\n",
    "from composer.core.types import Dataset\n",
    "from composer.loggers import WandBLogger\n",
    "from composer.optim import DecoupledAdamW\n",
    "from composer.optim.scheduler import (\n",
    "    ConstantWithWarmupScheduler,\n",
    "    CosineAnnealingWithWarmupScheduler,\n",
    "    LinearWithWarmupScheduler,\n",
    ")\n",
    "from src.scheduler import WarmupStableDecayScheduler\n",
    "from composer.utils import dist, reproducibility\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goreski/miniconda/envs/bert24/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from glue dataset:\n",
      "{'question': 'When did the third Digimon series begin?', 'sentence': 'Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.', 'label': 1, 'idx': 0}\n",
      "{'question': 'Which missile batteries often have individual launchers several kilometres from one another?', 'sentence': 'When MANPADS is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.', 'label': 1, 'idx': 1}\n",
      "{'question': \"What two things does Popper argue Tarski's theory involves in an evaluation of truth?\", 'sentence': 'He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.', 'label': 0, 'idx': 2}\n",
      "{'question': 'What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians?', 'sentence': 'On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854.', 'label': 0, 'idx': 3}\n",
      "{'question': 'What famous palace is located in London?', 'sentence': \"London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0Â° longitude, and GMT).\", 'label': 1, 'idx': 4}\n",
      "dict_keys(['labels', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'])\n",
      "{'labels': tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]), 'idx': tensor([104315,  74962,  80369,  99703,  15104,  60806,  11905,  75662,  22350,\n",
      "         32578,  31140,  16361,  24973,  90682,  91633,  87791]), 'input_ids': tensor([[ 101, 2129, 2146,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 1996, 5274,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 2055, 2129,  ...,    0,    0,    0],\n",
      "        [ 101, 2108, 2485,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "def build_my_dataloader(cfg: DictConfig, device_batch_size: int):\n",
    "    \"\"\"Create a dataloader for classification.\n",
    "\n",
    "    **Modify this function to train on your own dataset!**\n",
    "\n",
    "    This function is provided as a starter code to simplify fine-tuning a BERT\n",
    "    classifier on your dataset. We'll use the dataset for QNLI (one of the\n",
    "    GLUE tasks) as a demonstration.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): An omegaconf config that houses all the configuration\n",
    "            variables needed to instruct dataset/dataloader creation.\n",
    "        device_batch_size (int): The size of the batches that the dataloader\n",
    "            should produce.\n",
    "\n",
    "    Returns:\n",
    "        dataloader: A dataloader set up for use of the Composer Trainer.\n",
    "    \"\"\"\n",
    "    # As a demonstration, we're using the QNLI dataset from the GLUE suite\n",
    "    # of tasks.\n",
    "    #\n",
    "    # Note: We create our dataset using the `data_module.create_glue_dataset` utility\n",
    "    #   defined in `./src/glue/data.py`. If you inspect that code, you'll see\n",
    "    #   that we're taking some extra steps so that our dataset yields examples\n",
    "    #   that follow a particular format. In particular, the raw text is\n",
    "    #   tokenized and some of the data columns are removed. The result is that\n",
    "    #   each example is a dictionary with the following:\n",
    "    #\n",
    "    #     - 'input_ids': the tokenized raw text\n",
    "    #     - 'label': the target class that the text belongs to\n",
    "    #     - 'attention_mask': a list of 1s and 0s to indicate padding\n",
    "    #\n",
    "    # When you set up your own dataset, it should handle tokenization to yield\n",
    "    # examples with a similar structure!\n",
    "    #\n",
    "    # REPLACE THIS WITH YOUR OWN DATASET:\n",
    "    dataset = data_module.create_glue_dataset(\n",
    "        task=\"qnli\",\n",
    "        split=cfg.split,\n",
    "        tokenizer_name=cfg.tokenizer_name,\n",
    "        max_seq_length=cfg.max_seq_len,\n",
    "    )\n",
    "\n",
    "    dataset = cast(Dataset, dataset)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        # As an alternative to formatting the examples inside the dataloader,\n",
    "        # you can write a custom data collator to do that instead.\n",
    "        collate_fn=transformers.default_data_collator,\n",
    "        batch_size=device_batch_size,\n",
    "        sampler=dist.get_sampler(dataset, drop_last=cfg.drop_last, shuffle=cfg.shuffle),\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=cfg.get(\"pin_memory\", True),\n",
    "        prefetch_factor=cfg.get(\"prefetch_factor\", 2),\n",
    "        persistent_workers=cfg.get(\"persistent_workers\", True),\n",
    "        timeout=cfg.get(\"timeout\", 0),\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "yaml_path, args_list = \"yamls/test/sequence_classification.yaml\", sys.argv[2:]\n",
    "with open(\"yamls/defaults.yaml\") as f:\n",
    "    default_cfg = om.load(f)\n",
    "with open(yaml_path) as f:\n",
    "    yaml_cfg = om.load(f)\n",
    "cli_cfg = om.from_cli(args_list)\n",
    "cfg = om.merge(default_cfg, yaml_cfg, cli_cfg)\n",
    "cfg = cast(DictConfig, cfg)  # for type checking\n",
    "\n",
    "train_loader = build_my_dataloader(\n",
    "    cfg.train_loader,\n",
    "    cfg.global_train_batch_size // dist.get_world_size(),\n",
    ")\n",
    "\n",
    "#get one data sample from the train_loader\n",
    "data = next(iter(train_loader))\n",
    "# Print data keys only\n",
    "print(data.keys())\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
